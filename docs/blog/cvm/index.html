<!doctype html><html lang=en><head><meta charset=utf-8><meta content="text/html; charset=UTF-8" http-equiv=content-type><meta content="width=device-width,initial-scale=1" name=viewport><title>Implementing CVM For Fun and Fun</title><link href="https://blinsay.com/ atom.xml" rel=alternate title=RSS type=application/atom+xml><link href=https://blinsay.com/blog/cvm/ rel=canonical><meta content=website property=og:type><meta content=https://blinsay.com/blog/cvm/ property=og:url><link href=https://blinsay.com/favicon-16x16.png rel=icon size=16x16 type=image/png><link href=https://blinsay.com/favicon-32x32.png rel=icon size=32x32 type=image/png><link href=https://blinsay.com/style.css rel=stylesheet><link href=https://blinsay.com/atom.xml rel=alternate title=atom type=application/atom+xml><body theme=auto><div class=w><header><nav><a href=/>home</a><a href=/blog>posts</a></nav><h1>Implementing CVM For Fun and Fun</h1><p class=post-meta><time datetime=2024-05-28>2024-05-28</time></header><main aria-label=Content class=page-content><p>There's been a new Knuth paper about an distinct-values sketch floating around my corner of the internet. Despite having not interacted with a sketch professionally in quite a long time, sketches are still <a href="https://www.youtube.com/watch?v=y3fTaxA8PkU">near and dear to my heart</a> and this felt like a fun opportunity to implement something new.<p>For the unfamiliar, probabilistic sketching is cheating at data structures by using statistics. In return for accepting some amount of error, and usually a bounded amount, you get to break the fundamental laws of computer science. A <a href=https://en.wikipedia.org/wiki/Bloom_filter>Bloom filter</a> is the most famous example, but sketches like HyperLogLog and CountMinSketch have been showing up in industry with some regularity.<p>The sketch in Knuth's note is a sketch that, like <a href=https://dmtcs.episciences.org/3545/pdf>HyperLogLog</a>, gives an estimator for the distinct element problem, aka the "how big is my set" problem. The new algorithm is interesting in that it doesn't solve the problem with tighter bounds or while using less space or really offer any improvement over HLLs and other state-of-the-art sketches. It's intended as a teaching tool, and is an algorithm that students can implement and can write coherent proofs about without the <a href=https://mathworld.wolfram.com/MellinTransform.html>graduate level math skills</a> that are table stakes for proving meaningful bounds on other sketches.<hr><p>What Knuth calls CVM was originally proposed by <a href=https://arxiv.org/abs/2301.10191>Sourav Chakraborty, N. V. Vinodchandran, and Kuldeep S. Meel</a> in a paper that explicitly targets teaching. The original paper doesn't even focus on implementation and data structures, it aims itself directly at students looking to write proofs about the algorithm. The paper works through three verisons of this algorithm and spends most of it's time on proving properties of each iteration, and building on the proofs from previous iterations to prove things about the next.<p>All three versions of the algorithm work by keeping a set of elements seen in the stream until it hits some predetermined size threshold. At that point, the algorithm randomly discards elements of the set with some probability. The first iteration of the algorithm discards every element with probability 1/2 and terminates if the set is ever empty. The second iteration simply removes the termination condition. The third and final iteration introduces randomness, and uses that randomness to discard from the set of observed elements.<p>On its own, this paper would have been deeply unapproachable to anyone who was not a student of probability theory - it's <em>very</em> focused on the math. As someone who's been away from that kind of math for more than a decade, I absolutely can't follow the proofs here. Implementation and data structures are clearly an afterthought - the third iteration of the algorithm depends on keeping around an infinitely growing string of random bits.<hr><p><a href=https://www-cs-faculty.stanford.edu/~knuth/papers/cvm-note.pdf>Along comes Donald Knuth</a>, with a celebration of this algorithm.<p>Knuth makes the whole thing much more approachable to the lay programmer. Knuth starts at the end with what alternately calls Algorithm D and CVM (named after the three original authors), and focuses on the algorithm and intuitive proof that it works before diving in. Knuth being Knuth, he also spends plenty of time on the data structures required to make CVM fast and efficient. After laying out the algorithm Knuth dives into proofs as well, building back up from simpler algorithms to proofs on the bounds of Algorithm D.<p>Interestingly, Knuth seems to believe that it's possible to prove tighter bounds on Algorithm D.<blockquote><p>To conclude this study, I had hoped to use Theorem D to sharpen the results of Corollary T, because the actual performance of Algorithm D in practice is noticeably better than the comparatively weak theoretical guarantees that are derivable from the coarser Algorithm D. Algorithm D is quite simple, so I believed that I’d be able to analyze its behavior without great difficulty. Alas, after several weeks of trying, I must confess that I’ve failed to get much further. Indeed, I think Algorithm D may well be the simplest algorithm that lies beyond my current ability to carry out a sharp analysis!</blockquote><p>Watch out, undergraduates.<hr><p>At an extremely high level, Knuth's CVM works by keeping a set of elements seen so far paired with a randomly sampled value he dubs a "volatility". Each (non-distinct) element in the stream is assigned a volatility, and is sampled if it has a lower volatility than any other element sampled so far, evicting the element paired with the highest volatility. On considering a new element, CVM updates a probability parameter <code>p</code> based on both the new incoming volatility and whether or not an element was evicted. At any point in time, the size of the sampled set divided by <code>p</code> is an unbiased estimator of the number of unique elements in the stream.<p>The high-level explanation glosses over important details in the algorithm that handle edge cases and correct for sampling bias, but I found that an actual implementation is as readable and concicse as the pseudocode Knuth presents for Algorithm D.<pre class=language-rust data-lang=rust style=background:#eff1f5;color:#4f5b66><code class=language-rust data-lang=rust><span style=color:#b48ead>struct </span><span>Cvm {
</span><span>    </span><span style=color:#bf616a>size</span><span>: </span><span style=color:#b48ead>usize</span><span>,
</span><span>    </span><span style=color:#bf616a>p</span><span>: </span><span style=color:#b48ead>f32</span><span>,
</span><span>    </span><span style=color:#bf616a>buf</span><span>: Treap,
</span><span>}
</span><span>
</span><span style=color:#b48ead>impl </span><span>Cvm {
</span><span>    </span><span style=color:#b48ead>pub fn </span><span style=color:#8fa1b3>new</span><span>(</span><span style=color:#bf616a>size</span><span>: </span><span style=color:#b48ead>usize</span><span>) -> </span><span style=color:#b48ead>Self </span><span>{
</span><span>        </span><span style=color:#a7adba>// D1 - p=1, buf is empty
</span><span>        </span><span style=color:#b48ead>Self </span><span>{
</span><span>            size,
</span><span>            p: </span><span style=color:#d08770>1.0</span><span>,
</span><span>            buf: Treap::new(),
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span style=color:#b48ead>pub fn </span><span style=color:#8fa1b3>insert</span><span>(&</span><span style=color:#b48ead>mut </span><span style=color:#bf616a>self</span><span>, </span><span style=color:#bf616a>key</span><span>: </span><span style=color:#b48ead>u32</span><span>) {
</span><span>        </span><span style=color:#b48ead>let</span><span> u: </span><span style=color:#b48ead>f32 </span><span>= rand::thread_rng().</span><span style=color:#96b5b4>gen</span><span>();
</span><span>
</span><span>        </span><span style=color:#a7adba>// D4 - if B contains the pair (a, u) delete it
</span><span>        </span><span style=color:#bf616a>self</span><span>.buf.</span><span style=color:#96b5b4>remove</span><span>(&key);
</span><span>
</span><span>        </span><span style=color:#a7adba>// D5 - if u >= p, bail. if there's room in the
</span><span>        </span><span style=color:#a7adba>// buffer, either because the buffer is still growing
</span><span>        </span><span style=color:#a7adba>// to its max or the same key just got removed,
</span><span>        </span><span style=color:#a7adba>// insert the new node.
</span><span>        </span><span style=color:#b48ead>if </span><span style=color:#bf616a>self</span><span>.p <= u {
</span><span>            </span><span style=color:#b48ead>return</span><span>;
</span><span>        }
</span><span>
</span><span>        </span><span style=color:#b48ead>if </span><span style=color:#bf616a>self</span><span>.buf.</span><span style=color:#96b5b4>len</span><span>() < </span><span style=color:#bf616a>self</span><span>.size {
</span><span>            </span><span style=color:#bf616a>self</span><span>.buf.</span><span style=color:#96b5b4>insert</span><span>(key, u);
</span><span>            </span><span style=color:#b48ead>return</span><span>;
</span><span>        }
</span><span>
</span><span>        </span><span style=color:#a7adba>// D6 - based on the value of u, either swap the new
</span><span>        </span><span style=color:#a7adba>// node into the buffer or update the value of p.
</span><span>        </span><span style=color:#b48ead>let </span><span>(key_max, u_max) = </span><span style=color:#bf616a>self</span><span>.buf.</span><span style=color:#96b5b4>last</span><span>().</span><span style=color:#96b5b4>unwrap</span><span>();
</span><span>        </span><span style=color:#b48ead>if</span><span> u_max < u {
</span><span>            </span><span style=color:#bf616a>self</span><span>.p = u
</span><span>        } </span><span style=color:#b48ead>else </span><span>{
</span><span>            </span><span style=color:#bf616a>self</span><span>.buf.</span><span style=color:#96b5b4>remove</span><span>(&key_max);
</span><span>            </span><span style=color:#bf616a>self</span><span>.buf.</span><span style=color:#96b5b4>insert</span><span>(key, u);
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span style=color:#b48ead>pub fn </span><span style=color:#8fa1b3>estimate</span><span>(&</span><span style=color:#bf616a>self</span><span>) -> </span><span style=color:#b48ead>f32 </span><span>{
</span><span>        </span><span style=color:#a7adba>// at any point, D2 is a valid estimate
</span><span>        </span><span style=color:#bf616a>self</span><span>.buf.</span><span style=color:#96b5b4>len</span><span>() as </span><span style=color:#b48ead>f32 </span><span>/ </span><span style=color:#bf616a>self</span><span>.p
</span><span>    }
</span><span>}
</span></code></pre><p>The only step in Knuth's algorithm D that it doesn't make sense to represent explicitly in code is Step D3 - sampling the next element from a stream. In practice, you'll be doing that by processing an incoming stream of data from some external source.<hr><p>Knuth's implementation of the algorithm is <a href=https://cs.stanford.edu/~knuth/programs/cvm-estimates.w>available as a literate program</a> alongside his note. If you're <a href=https://tug.org/texlive/>feeling adventurous</a>, you can turn it into a stripped C file or a <a href=https://blinsay.com/blog/cvm/cvm-estimates.pdf>rendered PDF</a>.<p>I found Knuth's implementation difficult to follow. It freely mixes the CVM implementation with the guts of a hash-table implementation and a all of the treap building and searching code, none of which really matters for the correctness of the CVM algorithm itself. In my reading almost all of the complexity seems to come from the hash table and treap code.<p>I spent a while staring at the treap implementation before deciding to ignore it entirely. While the treap is extremely important to the implementation of the algorithm if you'd like it to be fast (and a very cool data structure) it has nothing to do with whether or not the implementation correct.<p>It turns out it's fairly easy to fake a treap with a sorted array:<pre class=language-rust data-lang=rust style=background:#eff1f5;color:#4f5b66><code class=language-rust data-lang=rust><span>#[</span><span style=color:#bf616a>derive</span><span>(Debug)]
</span><span style=color:#b48ead>struct </span><span>Treap {
</span><span>    </span><span style=color:#bf616a>data</span><span>: Vec&LTNode>,
</span><span>}
</span><span>
</span><span>#[</span><span style=color:#bf616a>derive</span><span>(Debug, Clone, Copy)]
</span><span style=color:#b48ead>struct </span><span>Node {
</span><span>    </span><span style=color:#bf616a>key</span><span>: </span><span style=color:#b48ead>u32</span><span>,
</span><span>    </span><span style=color:#bf616a>vol</span><span>: </span><span style=color:#b48ead>f32</span><span>,
</span><span>}
</span><span>
</span><span style=color:#b48ead>impl </span><span>Treap {
</span><span>    </span><span style=color:#b48ead>fn </span><span style=color:#8fa1b3>new</span><span>() -> </span><span style=color:#b48ead>Self </span><span>{
</span><span>        </span><span style=color:#b48ead>Self </span><span>{ data: Vec::new() }
</span><span>    }
</span><span>
</span><span>    </span><span style=color:#b48ead>fn </span><span style=color:#8fa1b3>len</span><span>(&</span><span style=color:#bf616a>self</span><span>) -> </span><span style=color:#b48ead>usize </span><span>{
</span><span>        </span><span style=color:#bf616a>self</span><span>.data.</span><span style=color:#96b5b4>len</span><span>()
</span><span>    }
</span><span>
</span><span>   </span><span style=color:#b48ead>fn </span><span style=color:#8fa1b3>insert</span><span>(&</span><span style=color:#b48ead>mut </span><span style=color:#bf616a>self</span><span>, </span><span style=color:#bf616a>k</span><span>: </span><span style=color:#b48ead>u32</span><span>, </span><span style=color:#bf616a>v</span><span>: </span><span style=color:#b48ead>f32</span><span>) {
</span><span>        </span><span style=color:#bf616a>self</span><span>.</span><span style=color:#96b5b4>remove</span><span>(&k);
</span><span>
</span><span>        </span><span style=color:#b48ead>let</span><span> idx = </span><span style=color:#bf616a>self</span><span>.data.</span><span style=color:#96b5b4>partition_point</span><span>(|</span><span style=color:#bf616a>e</span><span>| e.vol < v);
</span><span>        </span><span style=color:#bf616a>self</span><span>.data.</span><span style=color:#96b5b4>insert</span><span>(idx, Node { key: k, vol: v });
</span><span>    }
</span><span>
</span><span>    </span><span style=color:#b48ead>fn </span><span style=color:#8fa1b3>remove</span><span>(&</span><span style=color:#b48ead>mut </span><span style=color:#bf616a>self</span><span>, </span><span style=color:#bf616a>k</span><span>: &</span><span style=color:#b48ead>u32</span><span>) {
</span><span>        </span><span style=color:#b48ead>let mut</span><span> i = </span><span style=color:#d08770>0</span><span>;
</span><span>        </span><span style=color:#b48ead>while</span><span> i < </span><span style=color:#bf616a>self</span><span>.data.</span><span style=color:#96b5b4>len</span><span>() {
</span><span>            </span><span style=color:#b48ead>if </span><span>&</span><span style=color:#bf616a>self</span><span>.data[i].key == k {
</span><span>                </span><span style=color:#bf616a>self</span><span>.data.</span><span style=color:#96b5b4>remove</span><span>(i);
</span><span>            } </span><span style=color:#b48ead>else </span><span>{
</span><span>                i += </span><span style=color:#d08770>1</span><span>;
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span style=color:#b48ead>fn </span><span style=color:#8fa1b3>last</span><span>(&</span><span style=color:#bf616a>self</span><span>) -> Option<(</span><span style=color:#b48ead>u32</span><span>, </span><span style=color:#b48ead>f32</span><span>)> {
</span><span>        </span><span style=color:#bf616a>self</span><span>.data.</span><span style=color:#96b5b4>last</span><span>().</span><span style=color:#96b5b4>map</span><span>(|</span><span style=color:#bf616a>n</span><span>| (n.key, n.vol))
</span><span>    }
</span><span>}
</span></code></pre><p>It's certainly not the prettiest or fastest thing in the world, but it lets us go test out CVM on some real data.<hr><p>To test, I ran my implementation against a stream of random numbers generated by the [<code>rand</code> crate]'s default RNG and comparing against a <code>HashSet</code> of the actual values. These tests generated elements until the HashSet hit 1,000,000 unique elements.<p>After 10 trials at each buffer size, my toy implementation appears to the naked eyeball to be close enough to Knuth's results for <code>Stream A1</code> - random data - that I'm happy to call this a working implementation.<p><img alt=cvm-results src=https://blinsay.com/blog/cvm/cvm-results.svg><p>Eyeballing this is as far as I went with analysis because the bounds that Knuth and the original authors prove are hard to comprehend, let alone to do napkin math about. I wouldn't believe most practicing software engineers if they told me they could intuit how a change in CVM buffer size (Knuth's <code>s</code>) changes the error. Compare that to an algorithm like HyperLogLog and it's error bound of <code>1.04/sqrt(m)</code>.<p>It really is quite astonishing that this is a good estimator. Given how simple the code is, and Knuth's ringing endorsement, I wouldn't be the least bit surprised if it makes it into more than a few CS textbooks.</main><footer></footer></div>